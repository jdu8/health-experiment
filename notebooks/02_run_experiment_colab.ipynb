{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Chatbot Memory Study - Google Colab\n",
    "\n",
    "This notebook runs the complete experiment with real LLMs in Google Colab.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU runtime (Runtime → Change runtime type → GPU)\n",
    "- ~15 GB GPU memory (use T4 or better)\n",
    "\n",
    "**What this does:**\n",
    "1. Clones the repository\n",
    "2. Installs dependencies\n",
    "3. Loads Mistral-7B models with 4-bit quantization\n",
    "4. Runs experiments comparing chatbot WITH vs WITHOUT memory\n",
    "5. Generates results and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup: Clone Repository & Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository (replace with your actual repo URL)\n",
    "!git clone https://github.com/yourusername/health-experiment.git\n",
    "%cd health-experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q torch torchvision torchaudio\n",
    "!pip install -q transformers>=4.35.0\n",
    "!pip install -q accelerate>=0.24.0\n",
    "!pip install -q bitsandbytes>=0.41.0\n",
    "!pip install -q sentencepiece>=0.1.99\n",
    "!pip install -q pyyaml numpy pandas python-dateutil tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify GPU and Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src to path\n",
    "sys.path.insert(0, str(Path.cwd()))\n",
    "\n",
    "# Check GPU\n",
    "print(\"=\" * 60)\n",
    "print(\"ENVIRONMENT CHECK\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"⚠️ WARNING: No GPU detected! LLM will be VERY slow on CPU.\")\n",
    "    print(\"   Go to Runtime → Change runtime type → Select GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test LLM Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.llm_utils import test_model_loading, get_device\n",
    "\n",
    "print(\"Testing LLM loading with a small model...\")\n",
    "print(\"(This will take 1-2 minutes)\\n\")\n",
    "\n",
    "device = get_device()\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Test with small GPT-2 model first\n",
    "success = test_model_loading(\"gpt2\")\n",
    "\n",
    "if success:\n",
    "    print(\"\\n✓ LLM system working correctly!\")\n",
    "else:\n",
    "    print(\"\\n✗ LLM test failed. Check errors above.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Initialize Simulation with LLMs\n",
    "\n",
    "This will load Mistral-7B with 4-bit quantization (~3.5GB GPU memory per model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.core.simulation import MedicalChatbotSimulation\n",
    "\n",
    "print(\"Initializing simulation with Mistral-7B models...\")\n",
    "print(\"(This will download models if first time - may take 5-10 minutes)\\n\")\n",
    "\n",
    "sim = MedicalChatbotSimulation(\n",
    "    config_path=\"config/experiment_config.yaml\",\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Simulation initialized!\")\n",
    "print(f\"  Patient model: {sim.patient_model_name}\")\n",
    "print(f\"  Doctor model: {sim.doctor_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Quick Test (1 day, both conditions)\n",
    "\n",
    "Test the system with a single day to verify everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running quick test: 1 day, both conditions...\\n\")\n",
    "\n",
    "# Test WITHOUT history\n",
    "print(\"=\" * 60)\n",
    "print(\"Test 1: WITHOUT history\")\n",
    "print(\"=\" * 60)\n",
    "results_without = sim.run_experiment(\n",
    "    patient_id=\"patient_001\",\n",
    "    num_days=1,\n",
    "    condition=\"without_history\",\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Test 1 complete:\")\n",
    "print(f\"  Conversations: {results_without['total_conversations']}\")\n",
    "print(f\"  Total turns: {results_without['total_turns']}\")\n",
    "\n",
    "# Test WITH history\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Test 2: WITH history\")\n",
    "print(\"=\" * 60)\n",
    "results_with = sim.run_experiment(\n",
    "    patient_id=\"patient_001\",\n",
    "    num_days=1,\n",
    "    condition=\"with_history\",\n",
    "    random_seed=43\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Test 2 complete:\")\n",
    "print(f\"  Conversations: {results_with['total_conversations']}\")\n",
    "print(f\"  Total turns: {results_with['total_turns']}\")\n",
    "\n",
    "print(\"\\n✓ Quick test passed! System is ready for full experiments.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. View Sample Conversation\n",
    "\n",
    "Let's look at what the LLMs generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load the most recent conversation\n",
    "conv_dir = Path(\"data/conversations/patient_001/without_history\")\n",
    "conv_files = sorted(conv_dir.glob(\"day_*.json\"))\n",
    "\n",
    "if conv_files:\n",
    "    with open(conv_files[-1], 'r') as f:\n",
    "        conv = json.load(f)\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"SAMPLE CONVERSATION: {conv['conversation_id']}\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Day: {conv['simulation_day']}\")\n",
    "    print(f\"Condition: {conv['condition']}\")\n",
    "    print(f\"Total turns: {len(conv['turns'])}\")\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    \n",
    "    # Show conversation\n",
    "    for i, turn in enumerate(conv['turns'], 1):\n",
    "        speaker = \"PATIENT\" if turn['speaker'] == 'patient' else \"CHATBOT\"\n",
    "        print(f\"\\n[{i}] {speaker}:\")\n",
    "        print(f\"{turn['message']}\")\n",
    "    \n",
    "    # Show metrics\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"METRICS\")\n",
    "    print(\"=\" * 60)\n",
    "    metrics = conv.get('metrics', {}).get('automatic', {})\n",
    "    print(f\"Bot questions asked: {metrics.get('bot_questions_asked', 0)}\")\n",
    "    print(f\"Info gathered: {metrics.get('key_info_gathered', [])}\")\n",
    "    print(f\"Info missed: {metrics.get('key_info_missed', [])}\")\n",
    "else:\n",
    "    print(\"No conversations found yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Run Full Experiment (7 days, both conditions)\n",
    "\n",
    "Now run the complete experiment. This will take ~30-45 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Running FULL experiment: 7 days, both conditions\")\n",
    "print(\"This will take approximately 30-45 minutes...\\n\")\n",
    "\n",
    "# Run for patient_001, 7 days, both conditions\n",
    "results = sim.run_full_experiment(\n",
    "    patient_ids=[\"patient_001\"],\n",
    "    conditions=[\"without_history\", \"with_history\"],\n",
    "    num_days=7\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EXPERIMENT COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Total experiments run: {len(results['results'])}\")\n",
    "print(f\"Successful: {sum(1 for r in results['results'] if 'error' not in r)}\")\n",
    "\n",
    "# Summary\n",
    "for result in results['results']:\n",
    "    if 'error' not in result:\n",
    "        print(f\"\\n{result['patient_id']} - {result['condition']}:\")\n",
    "        print(f\"  Conversations: {result['total_conversations']}\")\n",
    "        print(f\"  Total turns: {result['total_turns']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze Results\n",
    "\n",
    "Compare performance between WITH and WITHOUT history conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Load all conversations and extract metrics\n",
    "metrics_data = []\n",
    "\n",
    "for condition in [\"with_history\", \"without_history\"]:\n",
    "    conv_dir = Path(f\"data/conversations/patient_001/{condition}\")\n",
    "    \n",
    "    for conv_file in conv_dir.glob(\"day_*.json\"):\n",
    "        with open(conv_file, 'r') as f:\n",
    "            conv = json.load(f)\n",
    "        \n",
    "        metrics = conv.get('metrics', {}).get('automatic', {})\n",
    "        \n",
    "        metrics_data.append({\n",
    "            'condition': condition,\n",
    "            'day': conv['simulation_day'],\n",
    "            'turns': len(conv['turns']),\n",
    "            'bot_questions': metrics.get('bot_questions_asked', 0),\n",
    "            'redundant_questions': metrics.get('redundant_questions', 0),\n",
    "            'history_references': metrics.get('references_to_history', 0),\n",
    "            'info_gathered_count': len(metrics.get('key_info_gathered', [])),\n",
    "            'info_missed_count': len(metrics.get('key_info_missed', []))\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(metrics_data)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RESULTS COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Group by condition and calculate means\n",
    "summary = df.groupby('condition').agg({\n",
    "    'turns': 'mean',\n",
    "    'bot_questions': 'mean',\n",
    "    'redundant_questions': 'mean',\n",
    "    'history_references': 'mean',\n",
    "    'info_gathered_count': 'mean',\n",
    "    'info_missed_count': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "print(\"\\nAverage per conversation:\")\n",
    "print(summary)\n",
    "\n",
    "# Calculate percentage difference\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with_hist = summary.loc['with_history']\n",
    "without_hist = summary.loc['without_history']\n",
    "\n",
    "print(f\"\\nRedundant questions:\")\n",
    "print(f\"  WITH history: {with_hist['redundant_questions']:.2f}\")\n",
    "print(f\"  WITHOUT history: {without_hist['redundant_questions']:.2f}\")\n",
    "\n",
    "print(f\"\\nHistory references:\")\n",
    "print(f\"  WITH history: {with_hist['history_references']:.2f}\")\n",
    "print(f\"  WITHOUT history: {without_hist['history_references']:.2f}\")\n",
    "\n",
    "print(f\"\\nInformation gathering efficiency:\")\n",
    "print(f\"  WITH history: {with_hist['info_gathered_count']:.2f} gathered, {with_hist['info_missed_count']:.2f} missed\")\n",
    "print(f\"  WITHOUT history: {without_hist['info_gathered_count']:.2f} gathered, {without_hist['info_missed_count']:.2f} missed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Download Results\n",
    "\n",
    "Download all data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Create zip of all results\n",
    "print(\"Creating results archive...\")\n",
    "shutil.make_archive('experiment_results', 'zip', 'data')\n",
    "\n",
    "print(\"Downloading results...\")\n",
    "files.download('experiment_results.zip')\n",
    "\n",
    "print(\"\\n✓ Download complete!\")\n",
    "print(\"\\nThe zip contains:\")\n",
    "print(\"  - All conversation logs (JSON)\")\n",
    "print(\"  - Patient state files\")\n",
    "print(\"  - Results summaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Clear GPU Memory (Optional)\n",
    "\n",
    "If you want to run again or free up GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.llm_utils import clear_model_cache\n",
    "import gc\n",
    "\n",
    "print(\"Clearing GPU memory...\")\n",
    "clear_model_cache()\n",
    "gc.collect()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated()/1e9:.2f} GB\")\n",
    "    print(f\"GPU memory cached: {torch.cuda.memory_reserved()/1e9:.2f} GB\")\n",
    "\n",
    "print(\"\\n✓ Memory cleared!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
